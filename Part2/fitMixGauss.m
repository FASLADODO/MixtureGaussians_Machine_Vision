function mixGaussEst = fitMixGauss(data,k);
        
[nDim nData] = size(data);
%MAIN E-M ROUTINE 
%there are nData data points, and there is a hidden variable associated
%with each.  If the hidden variable is 0 this indicates that the data was
%generated by the first Gaussian.  If the hidden variable is 1 then this
%indicates that the hidden variable was generated by the second Gaussian
%etc.

postHidden = zeros(k, nData);

%in the E-M algorithm, we calculate a complete posterior distribution over
%the (nData) hidden variables in the E-Step.  In the M-Step, we
%update the parameters of the Gaussians (mean, cov, w).  

mixGaussEst = initializeGaussians(data,k);

%calculate current likelihood
logLike = getMixGaussLogLike(data,mixGaussEst,k);
prevll = logLike;
fprintf('Log Likelihood Iter 0 : %4.3f\n',logLike);

l = zeros(k,1);

nIter = 40;
for (cIter = 1:nIter)
   % ===================== =====================
   %Expectation step
   % ===================== =====================
   
   for (cData = 1:nData)
        %TO DO (g): fill in column of 'hidden' - calculate posterior probability that
        %this data point came from each of the Gaussians
        for h = 1:k
            l(h) = mixGaussEst.weight(h)*calcGaussianProb(data(:,cData),mixGaussEst.mean(:,h),mixGaussEst.cov(:,:,h));
        end
        postHidden(:,cData) = l/(sum(l));
   end;
   
   %Maximization Step
   sum_resp1 = sum(transpose(postHidden));
   %for each constituent Gaussian
   for (cGauss = 1:k) 
        %TO DO (h):  Update weighting parameters mixGauss.weight based on the total
        %posterior probability associated with each Gaussian. Replace this:
        mixGaussEst.weight(cGauss) = sum_resp1(cGauss)/nData; 
   
        %TO DO (i):  Update mean parameters mixGauss.mean by weighted average
        %where weights are given by posterior probability associated with
        %Gaussian.
        resp = repmat(postHidden(cGauss,:),nDim,1);
        m = resp .* data;
        sum_resp = sum(transpose(m));
        sum_post_hidden = sum(postHidden(cGauss,:));
        mixGaussEst.mean(:,cGauss) = sum_resp/sum_post_hidden;
        
        %TO DO (j):  Update covarance parameter based on weighted average of
        %square distance from update mean, where weights are given by
        %posterior probability associated with Gaussian
        mixGaussEst.cov(:,:,cGauss) = resp.*(data-mixGaussEst.mean(:,cGauss))*transpose(data-mixGaussEst.mean(:,cGauss))/sum_post_hidden;
   end;
   

   %calculate the log likelihood
   logLike = getMixGaussLogLike(data,mixGaussEst,k);
   if logLike == prevll
       fprintf('ENTERED');
       return;
   else
       prevll = logLike;
   end
   fprintf('Log Likelihood Iter %d : %4.3f\n',cIter,logLike);
end;
end


%==========================================================================
%==========================================================================

%the goal of this routine is to calculate the log likelihood for the whole
%data set under a mixture of Gaussians model. We calculate the log as the
%likelihood will probably be a very small number that Matlab may not be
%able to represent.
function logLike = getMixGaussLogLike(data,mixGaussEst,k);

%find total number of data items
nData = size(data,2);

%initialize log likelihoods
logLike = 0;

%run through each data item
for(cData = 1:nData)
    thisData = data(:,cData);    
    %TO DO - calculate likelihood of this data point under mixture of
    %Gaussians model.
    like = 0;
    for (h = 1:k)
        like = like + calcGaussianProb(thisData,mixGaussEst.mean(:,h),mixGaussEst.cov(:,:,h))*mixGaussEst.weight(h);
    end;
    %add to total log like
    logLike = logLike+log(like);        
end;
end

%==========================================================================
%==========================================================================

%the goal of this routine is to evaluate a Gaussian likleihood
function like = calcGaussianProb(data,gaussMean,gaussCov)

[nDim nData] = size(data);
A = 1/((2*pi)^(nData/2)*det(gaussCov)^(0.5));
B = exp(-0.5*transpose(data-gaussMean)*inv(gaussCov)*(data-gaussMean));

like = A*B;
end



function mixGaussEst = initializeGaussians(data,k);
    [nDim,nData] = size(data);
    mixGaussEst.d = nDim;
    mixGaussEst.k = k;
    mixGaussEst.weight = (1/k)*ones(1,k);
    mixGaussEst.mean = zeros(nDim,k);
    mixGaussEst.cov = zeros(nDim,nDim,k);
    for iter = 1:k
        i = round(rand*nData);
        mixGaussEst.mean(:,iter) = data(:,i);
        for dim = 1:nDim
            mixGaussEst.cov(dim,dim,iter) = sum(sum((data-mixGaussEst.mean(dim,iter)).^2))/nData;
        end
    end
end

